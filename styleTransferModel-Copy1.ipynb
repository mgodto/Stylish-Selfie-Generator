{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "CONTENT_DIRS = './dataset/img_align_celeba_png/img_align_celeba_png'\n",
    "STYLE_DIRS = './dataset/test'\n",
    "\n",
    "# VGG19 was trained by Caffe which converted images from RGB to BGR,\n",
    "# then zero-centered each color channel with respect to the ImageNet \n",
    "# dataset, without scaling.  \n",
    "IMG_MEANS = np.array([103.939, 116.779, 123.68]) # BGR\n",
    "\n",
    "IMG_SHAPE = (224, 224, 3) # training image shape, (h, w, c)\n",
    "SHUFFLE_BUFFER = 1000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = 12000 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(STYLE_DIRS+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_files(dir, num, pattern='**/*.jpg'):\n",
    "    '''Samples files in a directory using the reservoir sampling.'''\n",
    "\n",
    "    paths = Path(dir).glob(pattern) # list of Path objects\n",
    "    sampled = []\n",
    "    for i, path in enumerate(paths):\n",
    "        if i < num:\n",
    "            sampled.append(path) \n",
    "        else:\n",
    "            s = random.randint(0, i)\n",
    "            if s < num:\n",
    "                sampled[s] = path\n",
    "    return sampled\n",
    "\n",
    "def plot_images(dir, row, col, pattern):\n",
    "    paths = sample_files(dir, row*col, pattern)\n",
    "\n",
    "    plt.figure(figsize=(2*col, 2*row))\n",
    "    for i in range(row*col):\n",
    "        im = Image.open(paths[i])\n",
    "        w, h = im.size\n",
    "\n",
    "        plt.subplot(row, col, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(f'{w}x{h}')\n",
    "    plt.show()\n",
    "\n",
    "#print('Sampled content images:')\n",
    "#plot_images(CONTENT_DIRS, 4, 8, pattern='*.png')\n",
    "\n",
    "#print('Sampled style images:')\n",
    "#plot_images(STYLE_DIRS, 4, 8, pattern='*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(dir_path, min_shape=None):\n",
    "#     paths = Path(dir_path).glob('**/*.jpg')\n",
    "#     deleted  = 0\n",
    "#     for path in paths:\n",
    "#         try:\n",
    "#             # Make sure we can decode the image\n",
    "#             im = tf.io.read_file(str(path.resolve()))\n",
    "#             im = tf.image.decode_jpeg(im)\n",
    "\n",
    "#             # Remove grayscale images \n",
    "#             shape = im.shape\n",
    "#             if shape[2] < 3:\n",
    "#                 path.unlink()\n",
    "#                 deleted += 1\n",
    "\n",
    "#             # Remove small images\n",
    "#             if min_shape is not None:\n",
    "#                 if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "#                     path.unlink()\n",
    "#                     deleted += 1\n",
    "#         except Exception as e:\n",
    "#             path.unlink()\n",
    "#             deleted += 1\n",
    "#     return deleted\n",
    "\n",
    "# # for dir in CONTENT_DIRS:\n",
    "# #     deleted = clean(dir)\n",
    "# # print(f'#Deleted content images: {deleted}')\n",
    "\n",
    "# for dir in STYLE_DIRS:\n",
    "#     deleted = clean(dir)\n",
    "# print(f'#Deleted style images: {deleted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dir_path, min_shape=None):\n",
    "    paths = glob.glob(dir_path+'/*.jpg')\n",
    "    deleted  = 0\n",
    "    deleted_path = []\n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Make sure we can decode the image\n",
    "            im = tf.io.read_file(str(path.resolve()))\n",
    "            im = tf.image.decode_jpeg(im)\n",
    "\n",
    "            # Remove grayscale images \n",
    "            shape = im.shape\n",
    "            if shape[2] < 3:\n",
    "                deleted_path.append(path)\n",
    "                deleted += 1\n",
    "\n",
    "            # Remove small images\n",
    "            if min_shape is not None:\n",
    "                if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "                    deleted_path.append(path)\n",
    "                    deleted += 1\n",
    "        except Exception as e:\n",
    "            deleted_path.append(path)\n",
    "            deleted += 1\n",
    "    return deleted, deleted_path\n",
    "for dir in STYLE_DIRS:\n",
    "    deleted, deleted_path = clean(dir)\n",
    "for p in deleted_path:\n",
    "    os.remove(p)\n",
    "# print(f'#Deleted style images: {deleted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(path, init_shape=(448, 448)):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "def preprocess_style(path, init_shape=(448, 448)):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (224,224))\n",
    "    #image = tf.image.random_crop(image, size=IMG_SHAPE)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "\n",
    "def np_image(image):\n",
    "    image += (103.939, 116.779, 123.68) # BGR means\n",
    "    image = image[..., ::-1] # BGR to RGB\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    image = tf.cast(image, dtype='uint8')\n",
    "    return image.numpy()\n",
    "\n",
    "def build_dataset(num_gpus=1):\n",
    "    paths = glob.glob(CONTENT_DIRS+'/*.png')\n",
    "    c_paths = []\n",
    "    num = 40000\n",
    "    for i, path in enumerate(paths):\n",
    "        if i < num:\n",
    "            c_paths.append(path) \n",
    "        else:\n",
    "            s = random.randint(0, i)\n",
    "            if s < num:\n",
    "                c_paths[s] = path\n",
    "    \n",
    "    s_paths = []\n",
    "    s_paths += glob.glob(STYLE_DIRS+'/*.jpg')\n",
    "    \n",
    "    print(f'Building dataset from {len(c_paths):,} content images and {len(s_paths):,} style images... ', end='')\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    c_ds = tf.data.Dataset.from_tensor_slices(c_paths)\n",
    "    c_ds = c_ds.map(preprocess_content, num_parallel_calls=AUTOTUNE)\n",
    "    c_ds = c_ds.repeat()\n",
    "    c_ds = c_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    s_ds = tf.data.Dataset.from_tensor_slices(s_paths)\n",
    "    s_ds = s_ds.map(preprocess_style, num_parallel_calls=AUTOTUNE)\n",
    "    s_ds = s_ds.repeat()\n",
    "    s_ds = s_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((c_ds, s_ds))\n",
    "    ds = ds.batch(BATCH_SIZE * num_gpus)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print('done')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nds = build_dataset()\\nc_batch, s_batch = next(iter(ds.take(1)))\\n\\nprint('Content batch shape:', c_batch.shape)\\nprint('Style batch shape:', s_batch.shape)\\n\\nplt.figure(figsize=(8, 4))\\n\\nplt.subplot(1, 2, 1)\\nplt.imshow(np_image(c_batch[0]))\\nplt.grid(False)\\nplt.xticks([])\\nplt.yticks([])\\nplt.xlabel('Content')\\n\\nplt.subplot(1, 2, 2)\\nplt.imshow(np_image(s_batch[0]))\\nplt.grid(False)\\nplt.xticks([])\\nplt.yticks([])\\nplt.xlabel('Style')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ds = build_dataset()\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "\n",
    "print('Content batch shape:', c_batch.shape)\n",
    "print('Style batch shape:', s_batch.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np_image(c_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Content')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np_image(s_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Style')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    # TODO\n",
    "    def __init__(self, name):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs[0], inputs[1]\n",
    "        \n",
    "        mean_x, val_x = tf.nn.moments(x, [1,2], keepdims=True)\n",
    "        mean_y, val_y = tf.nn.moments(y, [1,2], keepdims=True)\n",
    "        return tf.nn.batch_normalization(x,mean_x, val_x, mean_y, tf.sqrt(val_y), self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbitraryStyleTransferNet(tf.keras.Model):\n",
    "    CONTENT_LAYER = 'block4_conv1'\n",
    "    STYLE_LAYERS = ('block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1')\n",
    "\n",
    "    @staticmethod\n",
    "    def declare_decoder():\n",
    "        a_input = tf.keras.Input(shape=(28, 28, 512), name='input_adain')\n",
    "\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(a_input)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        output = tf.keras.layers.Conv2DTranspose(3, 3, padding='same')(h)\n",
    "\n",
    "        return tf.keras.Model(inputs=a_input, outputs=output, name='decoder')\n",
    "  \n",
    "    def __init__(self,\n",
    "                 img_shape=(224, 224, 3),\n",
    "                 content_loss_weight=1.5,\n",
    "                 style_loss_weight=10,\n",
    "                 name='arbitrary_style_transfer_net',\n",
    "                 **kwargs):\n",
    "        super(ArbitraryStyleTransferNet, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.content_loss_weight = content_loss_weight\n",
    "        self.style_loss_weight = style_loss_weight\n",
    "        \n",
    "        vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=img_shape)\n",
    "        vgg19.trainable = False\n",
    "\n",
    "        c_output = [vgg19.get_layer(ArbitraryStyleTransferNet.CONTENT_LAYER).output]\n",
    "        s_outputs = [vgg19.get_layer(name).output for name in ArbitraryStyleTransferNet.STYLE_LAYERS]\n",
    "        self.vgg19 = tf.keras.Model(inputs=vgg19.input, outputs=c_output+s_outputs, name='vgg19')\n",
    "        self.vgg19.trainable = False\n",
    "\n",
    "        self.adain = AdaIN(name='adain')\n",
    "        self.decoder = ArbitraryStyleTransferNet.declare_decoder()\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        c_batch, s_batch = inputs\n",
    "\n",
    "        c_enc = self.vgg19(c_batch)\n",
    "        c_enc_c = c_enc[0]\n",
    "\n",
    "        s_enc = self.vgg19(s_batch)\n",
    "        s_enc_c = s_enc[0]\n",
    "        s_enc_s = s_enc[1:] \n",
    "        \n",
    "        # normalized_c is the output of AdaIN layer\n",
    "        normalized_c = self.adain((c_enc_c, s_enc_c))\n",
    "        output = self.decoder(normalized_c)\n",
    "\n",
    "        # Calculate loss\n",
    "        out_enc = self.vgg19(output)\n",
    "        out_enc_c = out_enc[0]\n",
    "        out_enc_s = out_enc[1:]\n",
    "\n",
    "        loss_c = tf.reduce_mean(tf.math.squared_difference(out_enc_c, normalized_c))\n",
    "        self.add_loss(self.content_loss_weight * loss_c)\n",
    "        \n",
    "        loss_s = 0\n",
    "        for o, s in zip(out_enc_s, s_enc_s):    \n",
    "            o_mean, o_var = tf.nn.moments(o, axes=(1,2), keepdims=True)\n",
    "            o_std = tf.sqrt(o_var + self.adain.epsilon)\n",
    "\n",
    "            s_mean, s_var = tf.nn.moments(s, axes=(1,2), keepdims=True)\n",
    "            s_std = tf.sqrt(s_var + self.adain.epsilon)\n",
    "\n",
    "            loss_mean = tf.reduce_mean(tf.math.squared_difference(o_mean, s_mean))\n",
    "            loss_std = tf.reduce_mean(tf.math.squared_difference(o_std, s_std))\n",
    "\n",
    "            loss_s += loss_mean + loss_std\n",
    "        self.add_loss(self.style_loss_weight * loss_s)\n",
    "\n",
    "        return output, c_enc_c, normalized_c, out_enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from 1,286 content images and 23,585 style images... done\n",
      "Input shape: ((32, 224, 224, 3), (32, 224, 224, 3))\n",
      "Output shape: (32, 224, 224, 3)\n",
      "Init. content loss: 2,904,891.50, style loss: 10,860,589.00\n",
      "Model: \"arbitrary_style_transfer_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           [(None, 28, 28, 512), (No 3505728   \n",
      "_________________________________________________________________\n",
      "ada_in (AdaIN)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 224, 224, 3)       3505219   \n",
      "=================================================================\n",
      "Total params: 7,010,947\n",
      "Trainable params: 3,505,219\n",
      "Non-trainable params: 3,505,728\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ds = build_dataset()\n",
    "model = ArbitraryStyleTransferNet(img_shape=IMG_SHAPE)\n",
    "\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "print(f'Input shape: ({c_batch.shape}, {s_batch.shape})')\n",
    "output, *_ = model((c_batch, s_batch))\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Init. content loss: {model.losses[0]:,.2f}, style loss: {model.losses[1]:,.2f}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the following cell if it is the first time running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKP_DIR = 'checkpoints'\n",
    "model.save_weights(os.path.join(CKP_DIR, f'ckpt_{0}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from epoch 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=5e-4)\n",
    "c_loss_metric, s_loss_metric = tf.keras.metrics.Mean(), tf.keras.metrics.Mean()\n",
    "\n",
    "CKP_DIR = 'checkpoints'\n",
    "init_epoch = 1\n",
    "\n",
    "ckp = tf.train.latest_checkpoint(CKP_DIR)\n",
    "if ckp:\n",
    "    model.load_weights(ckp)\n",
    "    init_epoch = int(ckp.split('_')[-1]) + 1\n",
    "    print(f'Training from epoch {init_epoch-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model(inputs)\n",
    "        c_loss, s_loss = model.losses\n",
    "        loss = c_loss + s_loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    c_loss_metric(c_loss)\n",
    "    s_loss_metric(s_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def plot_outputs(outputs, captions=None, col=5):\n",
    "    row = len(outputs)\n",
    "    plt.figure(figsize=(3*col, 3*row))\n",
    "    for i in range(col):\n",
    "        for j in range(row):\n",
    "            plt.subplot(row, col, j*col+i+1)\n",
    "            plt.imshow(np_image(outputs[j][i,...,:3]))\n",
    "            plt.grid(False)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            if captions is not None:\n",
    "                plt.xlabel(captions[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, init_epoch):\n",
    "    \n",
    "    for epoch in range(init_epoch, EPOCHS+1):\n",
    "        start = time.time()\n",
    "        print(f'Epoch {epoch:>2}/{EPOCHS}')\n",
    "        for step, inputs in enumerate(ds.take(STEPS_PER_EPOCH)):\n",
    "            train_step(inputs)\n",
    "            print(f'{step+1:>5}/{STEPS_PER_EPOCH} - loss: {c_loss_metric.result()+s_loss_metric.result():,.2f} - content loss: {c_loss_metric.result():,.2f} - style loss: {s_loss_metric.result():,.2f}', end='\\r') \n",
    "\n",
    "        print()\n",
    "        model.save_weights(os.path.join(CKP_DIR, f'ckpt_{epoch}'))\n",
    "        c_loss_metric.reset_states()\n",
    "        s_loss_metric.reset_states()\n",
    "\n",
    "        output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "        plot_outputs((s_batch, c_batch, output), \n",
    "                     ('Style', 'Content', 'Trans'))\n",
    "        print('Epoch {} takes {} seconds'.format(epoch, time.time()-start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/30\n",
      "    2/375 - loss: 13,410,877.00 - content loss: 2,864,304.25 - style loss: 10,546,573.00\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-43f46d72e4ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-3019ac79d375>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, init_epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch:>2}/{EPOCHS}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{step+1:>5}/{STEPS_PER_EPOCH} - loss: {c_loss_metric.result()+s_loss_metric.result():,.2f} - content loss: {c_loss_metric.result():,.2f} - style loss: {s_loss_metric.result():,.2f}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflowGPUenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(ds, init_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
